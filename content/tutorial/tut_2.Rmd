---
title: "Introduction to Parallel Computing"
date: Nov 18, 2018
output: html_document
---
<style type="text/css">
body{ /* Normal  */
      font-size: 14px;
  }
h1 { /* Header 1 */
  font-size: 26px;
  color: DarkGreen;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkGreen;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

The goal of this series of tutorials is to teach ecologists on how to use parallel computing to perform null models. 
I thus assume that you already know what a null model is. :)
Otherwise, there is an excellent book on the subject (Gotelli and Graves, 1996) that is freely available to download [here](https://www.uvm.edu/~ngotelli/nullmodelspage.html).

Why use parallel processing? The simple answer is time.
If something takes less time done through parallel processing, why not do it? Computers nowadays have multi core processors with sufficient amount of memory available to run parallel processing. Instead of waiting a long time for a task to complete, one can divide the taks to run in multiple cores and thus obtain outputs much faster. 

Before deciding to parallelize your code, remember that there is a trade-off between performance and simplicity. It takes some time to set up the parallel cluster so if your code already runs fast it is not worth it. If your code repeats a similar task over and over (e.g., bootstraping) then it is recommended to parallize to  improve performance. Hence, parallel computing is ideal for null models.

While there are many good tutorials on parallel computing in R out there, I think it is better to provide a small introduction on how to set up a parallel cluster and how to start coding in parallel before jumping on the null models.

# R packages

There are many packages to run parallel processing in R. Here are a few examples:

* parallel
* snow
* doSNOW
* doParallel
* foreach



First install packages `install.packages("doParallel")`. The _doParallel_ library installs both _parallel_ and _foreach_ packages.

Then set up a parallel cluster.
Use the function `detectCores` from the _parallel_ library to detect the number of cores in your computer.
It is always recommended to leave one core free for the computer to run other processes.
For simplicity we will use 2 cores in this example.
```{r}
library(doParallel)
detectCores()
cl <- makeCluster(2, type = 'PSOCK')
```
This will create a _Parallel Socket Cluster_ (PSOCK). This type of cluster only starts with the base packages and thus you need to export additional variables, functions and packages to the cluster (see further below) if you need them in your parallel code.

You can also create a FORK cluster, which has better capabilities to manage memory and already contain all the variables from the local environment (i.e., no need for export) since all the cores share the same memory. For this, use: 
`cl <- makeCluster(2, type = 'FORK')` 
Note, however, that Windows does not support FORK clusters.

# foreach loops

An intuitive way to perform a parallel process is to use the foreach function which looks like a for loop but operates as a lapply() function. As a first example we will perform the foreach function in sequential mode by using %do%

```{r}
library(foreach)
foreach(i = 1:6) %do% {
  sqrt(i)
}

```
In the example above we applied `sqrt()` on each "i". 
By default, foreach returns the output in a list format. You can combine the results in different ways. For instance if you want a vector output you can add the argument ".combine = c". 
```{r}
foreach(i = 1:6, .combine = c) %do% {
  sqrt(i)
}

```
If you want to use the foreach in parallel mode, you need to change %do% to %dopar%.
You also need to register the cluster. Here we use the _doParallel_ package to do this
```{r}
registerDoParallel(cl)

foreach(i = 1:6, .combine = c) %dopar% {
  sqrt(i)
}

```
If you want to use a variable or object from the global environment in the parallel cluster, you first need to export it either using the `clusterExport` function, or using the .export argument in the foreach function. Below I use the argument
```{r}
var1 <-10
var2 <-5

foreach(i = 1:6, .combine = c, .export = c("var1","var2")) %dopar% {
  (var1+var2)*i
}

```
If you need to use a particular function in the parallel cluster, you also need to export it.
Here we use the `clusterExport` function.
```{r}
is_even <- function(x) x %% 2 == 0

clusterExport(cl, c("var1","var2","is_even"))

foreach(i = 1:6, .combine = c) %dopar% {
  is_even((var1+var2)*i)
}

```
If you need to use a function from a R package in the parallel cluster, you need to export the package in the cluster. This can be done by the argument ".package" in the foreach function or by using the `ClusterEvalQ` function.

Let's use the ChickWeigth dataset which contains the variables weigth, Time, Chick and Diet. 
We want to compute the growth of each chick (i.e., weigth ~ Time). We can use the dplyr package to apply the `lm` function to each chick in the dataset. If the output is a matrix and you want to combine the results by rows, you can use ".combine = rbind".
```{r}
data("ChickWeight")
clusterEvalQ(cl,library("dplyr"))
clusterExport(cl,"ChickWeight")

length(unique(ChickWeight$Chick))
#there are 50 unique chicks in this data

Growth_coeff<- foreach(i = 1:50, .combine = rbind, .packages = "dplyr") %dopar% {
  temp_data <- ChickWeight %>% filter(Chick == i)
  Growth = lm(weight ~ Time, data = temp_data)
  coefficients(Growth)
}

head(Growth_coeff)

```
In the example above the `combine = .rbind` argument will combine `coefficients(Growth)` into the `Growth_coeff` output. 

Finally, let's see how much time do we save by using the parallelized code.
In this case we will repeat the above code 100 times and use the `system.time` function to determine how much times it takes to run. 

First in sequential mode (%do%)
```{r}
trials <- 100
system.time({
  Growth_coeff <- foreach(icount(trials), .combine=rbind, .packages = "dplyr") %do% {
    
    temp_coeff<-matrix(numeric(),ncol=2,nrow=50)
    for(i in 1:50){
      temp_data <- ChickWeight %>% filter(Chick == i)
      Growth = lm(weight ~ Time, data = temp_data)
      temp_coeff[i,]=coefficients(Growth)
    }
   temp_coeff
  }
})
```
And now in parallel mode (%dopar%)
```{r}
trials <- 100
system.time({
  Growth_coeff <- foreach(icount(trials), .combine=rbind, .packages = "dplyr") %dopar% {
    
    temp_coeff<-matrix(numeric(),ncol=2,nrow=50)
    for(i in 1:50){
      temp_data <- ChickWeight %>% filter(Chick == i)
      Growth = lm(weight ~ Time, data = temp_data)
      temp_coeff[i,]=coefficients(Growth)
    }
   temp_coeff
  }
})

```
As you can see, the script ran in 10.31 seconds in sequential mode and 6.69 in parallel mode with only 2 cores.

