---
title: "Introduction to Parallel Computing"
author: "Renato Henriques-Silva"
date: "November 20, 2018"
output: html_document
---
<style type="text/css">
body{ /* Normal  */
    font-size: 14px;
}
h1 { /* Header 1 */
    font-size: 26px;
  color: DarkGreen;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkGreen;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>


## Why use parallel processing?

The goal of this series of tutorials is to teach ecologists on how to use parallel computing to perform null models. 
I thus assume that you already know what a null model is. :)

Otherwise, there is an excellent book on the subject (Gotelli and Graves, 1996) that is freely available to download [here](https://www.uvm.edu/~ngotelli/nullmodelspage.html).

Why use parallel processing? The simple answer is time.
If something takes less time done through parallel processing, why not do it? Computers nowadays have multi core processors with sufficient amount of memory available to run parallel processing. Instead of waiting a long time for a task to complete, one can divide the taks to run in multiple cores and thus obtain outputs much faster. 

Before deciding to parallelize your code, remember that there is a trade-off between performance and simplicity. It takes some time to set up the parallel cluster so if your code already runs fast it is not worth it. If your code repeats a similar task over and over (e.g., bootstraping) then it is recommended to parallize to  improve performance. Hence, parallel computing is ideal for null models.

While there are many good tutorials on parallel computing in R out there, I think it is better to provide a small introduction on how to set up a parallel cluster and how to start coding in parallel before jumping on the null models.

### R packages

There are many packages to run parallel processing in R. Here are a few examples:

* parallel
* snow
* doSNOW
* doParallel
* foreach

First install the _doParallel_ package `install.packages("doParallel")`, which also installs both _parallel_ and _foreach_ packages.
```{r message = FALSE}
library(doParallel)
```

Then set up a parallel cluster.
Use the function `detectCores` from the _parallel_ library to detect the number of cores in your computer.
```{r message = FALSE}
detectCores()
```
My computer has 4 cores.
It is always recommended to leave one core free for the computer to run other processes.
For simplicity we will use 2 cores in this tutorial.
```{r message = FALSE}
cl <- makeCluster(2, type = 'PSOCK')
```
This will create a _Parallel Socket Cluster_ (PSOCK). This type of cluster only starts with the base packages and thus you need to export additional variables, functions and packages to the cluster (see further below) if you need them in your parallelized code.

You can also create a FORK cluster, which has better capabilities to manage memory and already contain all the variables from the local environment (i.e., no need for export) since all the cores share the same memory. For this, use: 
`cl <- makeCluster(2, type = 'FORK')` 
Note, however, that Windows does not support FORK clusters.

### foreach loops

An intuitive way to perform a parallel process is to use the foreach function which looks like a for loop but operates as a lapply() function. As a first example we will perform the foreach function in sequential mode by using %do%

```{r message = FALSE}
foreach(i = 1:6) %do% {
  sqrt(i)
}

```
In the example above we applied `sqrt()` on each "i". 
By default, foreach returns the output in a list format. You can combine the results in different ways. For instance if you want a vector output you can add the argument ".combine = c". 
```{r message = FALSE}
foreach(i = 1:6, .combine = c) %do% {
  sqrt(i)
}

```
If you want to use the foreach in parallel mode, you need to change %do% to %dopar%.
You also need to register the cluster. Here we use the _doParallel_ package to do this.
```{r message = FALSE}
registerDoParallel(cl)

foreach(i = 1:6, .combine = c) %dopar% {
  sqrt(i)
}

```
If you want to use a variable or object from the global environment in the parallel cluster, you first need to export it either using the `clusterExport` function, or using the .export argument in the foreach function. Here I use the .export argument for exporting.
```{r message = FALSE}
var1 <-10
var2 <-5

foreach(i = 1:6, .combine = c, .export = c("var1","var2")) %dopar% {
  (var1+var2)*i
}

```
If you need to use a particular function in the parallel cluster, you also need to export it.
Here I use the `clusterExport` function.
```{r message = FALSE}
is_even <- function(x) x %% 2 == 0

clusterExport(cl, c("var1","var2","is_even"))

foreach(i = 1:6, .combine = c) %dopar% {
  is_even((var1+var2)*i)
}

```
If you need to use a function from a R package in the parallel cluster, you need to export the package in the cluster as well. This can be done by the argument ".package" in the foreach function or by using the `ClusterEvalQ` function.

Let's use the ChickWeigth dataset which contains the variables weigth, Time, Chick and Diet. 
We want to compute the growth of each chick (i.e., weigth ~ Time). We can use the dplyr package to subset each chick from the dataset and then apply the `lm` function to each one separately. Here we want the coefficients of the linear model as a result and we will combine them by rows by using ".combine = rbind".
```{r message = FALSE,  results = 'hide'}
data("ChickWeight")
clusterEvalQ(cl,library("dplyr"))
clusterExport(cl,"ChickWeight")
```

```{r message = FALSE}
length(unique(ChickWeight$Chick))
#there are 50 unique chicks in this data

Growth_coeff<- foreach(i = 1:50, .combine = rbind) %dopar% {
  temp_data <- ChickWeight %>% filter(Chick == i)
  Growth = lm(weight ~ Time, data = temp_data)
  coefficients(Growth)
}

head(Growth_coeff)

```
### Comparing execution time

In the example above the `combine = .rbind` argument combined `coefficients(Growth)` into the `Growth_coeff` output. 

Finally, let's see how much time do we save by using the parallelized code.
In this case we will repeat the above code 300 times and use the `system.time` function to determine how much times it takes to run. 

First in sequential mode (%do%)
```{r message = FALSE}
trials <- 300
system.time({
  Growth_coeff <- foreach(icount(trials), .combine=rbind, .packages = "dplyr") %do% {
    
    temp_coeff<-matrix(numeric(),ncol=2,nrow=50)
    for(i in 1:50){
      temp_data <- ChickWeight %>% filter(Chick == i)
      Growth = lm(weight ~ Time, data = temp_data)
      temp_coeff[i,]=coefficients(Growth)
    }
   temp_coeff
  }
})
```
And now in parallel mode (%dopar%)
```{r message = FALSE}
trials <- 300
system.time({
  Growth_coeff <- foreach(icount(trials), .combine=rbind, .packages = "dplyr") %dopar% {
    
    temp_coeff<-matrix(numeric(),ncol=2,nrow=50)
    for(i in 1:50){
      temp_data <- ChickWeight %>% filter(Chick == i)
      Growth = lm(weight ~ Time, data = temp_data)
      temp_coeff[i,]=coefficients(Growth)
    }
   temp_coeff
  }
})

```
